---
layout: layout
title: "Research Overview"
---

<center>  Here's my <a href="https://scholar.google.ca/citations?user=aH8AJu4AAAAJ&hl=en">Google Scholar</a>. Here's <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">how one should do research</a> according to <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>. Jump to <a href='#preprints'>preprints</a> or <a href='#publications'>publications.</a></center>


<!-- <div class="divider"></div> -->
<h2 id='preprints' class="page-heading">Preprints</h1>
  <div class="row">


    <div class="five columns">
  <img style="margin-top:0em" src="/images/research/nam.jpeg">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/2004.13912">ArXiv</a></td>
      <td><a href="https://twitter.com/nickfrosst/status/1255889440083447810?s=20">#TweePrint</a></td>
      <td><a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">Code</a></td>
<!--       <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td> -->
    </tr>
  </table>
</div>

    <div class="seven columns">

      <b><a href="https://arxiv.org/abs/2004.13912"> Neural Additive Models: Interpretable Machine Learning with Neural Nets </a></b>
      <p> <i> Rishabh Agarwal</i>, 
        <a href="https://www.nickfrosst.com/"> Nicholas Frosst</a>,
        <a href="http://pages.cs.wisc.edu/~zhangxz1123/"> Xuezhou Zhang</a>,
        <a href="https://www.microsoft.com/en-us/research/people/rcaruana/"> Rich Caruana</a>,
        <a href="https://www.cs.toronto.edu/~hinton/"> Geoffrey Hinton</a><br />
      </p>
    <p> We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility 
      of generalized additive models. NAMs are likely to be combined with deep learning methods in ways we don't foresee. 
      This is important because one of the key drawbacks of deep learning is intelligibility. 
    </p>
    </div>

  </div>

<div class="divider"></div>
<h2 id='publications' class="page-heading">Publications</h1>

<div class="row">
  
 <div class="row">
  <div class="five columns">
    <img style="margin-top:0em" src="/images/research/offline_rl.png">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1907.04543">ArXiv</a></td>
        <td><a href="https://offline-rl.github.io">Website</a></td>
        <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td>
      </tr>
    </table>
  </div>

    <div class="seven columns">

      <b><a href="https://offline-rl.github.io"> An Optimistic Perspective on Offline Reinforcement Learning </a></b>
      <p> <i> Rishabh Agarwal</i>,
        <a href="https://webdocs.cs.ualberta.ca/~dale/"> Dale Schuurmans</a>,
        <a href="https://norouzi.github.io/"> Mohammad Norouzi </a><br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> This paper presents an optimistic view that standard off-policy algorithms perform quite well in the fully
      off-policy / offline deep RL setting with large and diverse datasets. Furthermore, we find that simple algorithms inspired
      from supervised learning (<i>e.g.</i>, a random ensemble of <i>Q</i>-estimates) perform comparably to recent off-policy
      algorithms in the offline setting on Atari 2600 games. A previous version was titled "Striving for Simplicity in Off-Policy Deep Reinforcement Learning"
      and presented as a contributed talk at NeurIPS 2019 DRL workshop.
    </p>
    </div>
 </div>


  <div class="five columns">
<img style="margin-top:0em" src="/images/research/merl.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.07198">ArXiv</a></td>
    <td><a href="https://bit.ly/merl2019">Website</a></td>
    <td><a href="https://www.youtube.com/watch?v=_IXj6kXPdq8&feature=youtu.be">Talk</a></td>
    <!-- <td><a href="https://github.com/google-research/google-research/tree/master/meta_reward_learning">Code</a></td> -->
  </tr>
</table>
  </div>

  <div class="seven columns">

    <b><a href="https://agarwl.github.io/merl/">Learning to Generalize from Sparse and Underspecified Rewards</a></b>
    <p><i> Rishabh Agarwal</i>,
      <a href="https://crazydonkey200.github.io/"> Chen Liang</a>,
      <a href="https://webdocs.cs.ualberta.ca/~dale/"> Dale Schuurmans</a>,
      <a href="https://norouzi.github.io/"> Mohammad Norouzi</a> <br />
    ICML 2019 (<span style='color:red'>Short Oral</span>).
  </p>
 <!-- <p>Reinforcement learning (RL) has enabled remarkable success in addressing challenging tasks such as playing games such as Atari and Go, continuous control, and robotic learning. -->
  <p> Many real-world problems involve sparse and underspecified rewards, requiring a learning agent to generalize from limited feedback. In this work, we propose an effective exploration strategy for tackling sparse rewards and Meta Reward Learning to deal with underspecified rewards. These approaches provide substantial gains in language understanding tasks such as instruction following and semantic parsing.</p>
    </p>
  </div>
</div>




</div>
